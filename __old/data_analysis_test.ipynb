{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Factor Investing \n",
    "Fama and French 5 Faktor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "from lxml import etree\n",
    "import sqlite3\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import mplfinance as mpf\n",
    "from datetime import datetime, timedelta\n",
    "warnings.simplefilter(action='ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1) ETF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define URLs for different categories\n",
    "URLs = {\n",
    "    'MA': \"https://finance.yahoo.com/markets/etfs/most-active/\",\n",
    "    'TN': \"https://finance.yahoo.com/markets/etfs/top-performing/\",\n",
    "    'GA': \"https://finance.yahoo.com/markets/etfs/gainers/\",\n",
    "    'TP': \"https://finance.yahoo.com/markets/etfs/losers/\",\n",
    "    'TN': \"https://finance.yahoo.com/markets/etfs/trending/\",\n",
    "    'BH': \"https://finance.yahoo.com/markets/etfs/best-historical-performance/\",\n",
    "}\n",
    "\n",
    "def fetch_and_parse_ticker(url):\n",
    "    page = requests.get(url).content\n",
    "    tree = etree.HTML(page)\n",
    "    nodes = tree.xpath(\"//*[contains(concat(' ', @class, ' '), concat(' ', 'yf-138ga19', ' '))]\")\n",
    "    texts = [node.text for node in nodes]\n",
    "    cleaned = [text.strip() for text in texts if text and text.strip()]\n",
    "    tickers = [cleaned[i] for i in range(0, len(cleaned), 2)]\n",
    "    return tickers\n",
    "\n",
    "def create_table_if_not_exists(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS etf_categories\n",
    "    (date TEXT, ticker TEXT, category TEXT, timestamp TEXT)\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def data_exists_for_today(conn, date):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM etf_categories WHERE date = ?\", (date,))\n",
    "    count = cursor.fetchone()[0]\n",
    "    return count > 0\n",
    "\n",
    "def main():\n",
    "    current_date = dt.date.today().isoformat()\n",
    "    db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Create table if it doesn't exist\n",
    "    create_table_if_not_exists(conn)\n",
    "\n",
    "    # Check if data for today already exists\n",
    "    if data_exists_for_today(conn, current_date):\n",
    "        print(f\"Data for {current_date} already exists. Skipping insertion.\")\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    # Fetch and parse data for each category\n",
    "    data = {category: fetch_and_parse_ticker(url) for category, url in URLs.items()}\n",
    "\n",
    "    # Create a set of all unique tickers\n",
    "    all_tickers = set()\n",
    "    for tickers in data.values():\n",
    "        all_tickers.update(tickers)\n",
    "\n",
    "    # Create a DataFrame with all tickers and categories\n",
    "    df = pd.DataFrame(index=sorted(all_tickers), columns=URLs.keys())\n",
    "\n",
    "    # Fill the DataFrame\n",
    "    for category, tickers in data.items():\n",
    "        df[category] = df.index.isin(tickers).astype(int)\n",
    "\n",
    "    # Reset index to make 'ticker' a column\n",
    "    df = df.reset_index().rename(columns={'index': 'ticker'})\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    print(df.head())\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_path = os.path.join(os.getcwd(), 'etf_categories.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Data saved to {csv_path}\")\n",
    "\n",
    "    # Prepare data for insertion\n",
    "    data_to_insert = []\n",
    "    current_timestamp = dt.datetime.now().isoformat()\n",
    "    for category, tickers in data.items():\n",
    "        for ticker in tickers:\n",
    "            data_to_insert.append((current_date, ticker, category, current_timestamp))\n",
    "\n",
    "    # Insert data into database\n",
    "    cursor = conn.cursor()\n",
    "    query = '''\n",
    "    INSERT INTO etf_categories (date, ticker, category, timestamp)\n",
    "    VALUES (?, ?, ?, ?)\n",
    "    '''\n",
    "    cursor.executemany(query, data_to_insert)\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Data inserted successfully into {db_path}\")\n",
    "\n",
    "    # Check content of database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the SQL query\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT ticker\n",
    "    FROM etf_categories\n",
    "    WHERE date = ?\n",
    "      AND category = 'MA'\n",
    "    ORDER BY ticker;\n",
    "    \"\"\"\n",
    "    cursor.execute(query, (current_date,))\n",
    "\n",
    "    # Fetch and print the results\n",
    "    results = cursor.fetchall()\n",
    "    print(f\"\\nTickers in the 'MA' category for {current_date}:\")\n",
    "    for row in results:\n",
    "        print(row[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT DISTINCT ticker FROM etf_categories\")\n",
    "tickers = cursor.fetchall()\n",
    "\n",
    "def table_exists(ticker):\n",
    "    cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", (ticker,))\n",
    "    return cursor.fetchone() is not None\n",
    "\n",
    "def create_and_populate_table(ticker):\n",
    "    if table_exists(ticker):\n",
    "        print(f\"Table for {ticker} already exists. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    # Create the table if it doesn't exist\n",
    "    cursor.execute(f'''\n",
    "    CREATE TABLE IF NOT EXISTS \"{ticker}\" (\n",
    "        Date TEXT PRIMARY KEY,\n",
    "        Open REAL,\n",
    "        High REAL,\n",
    "        Low REAL,\n",
    "        Close REAL,\n",
    "        Volume INTEGER,\n",
    "        Dividends REAL,\n",
    "        Stock_Splits REAL\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "    # Calculate date range for the past year\n",
    "    end_date = datetime.now().date()\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "\n",
    "    # Fetch historical data\n",
    "    stock = yf.Ticker(ticker)\n",
    "    hist = stock.history(period=\"max\")\n",
    "\n",
    "    # Convert the index to datetime.date objects\n",
    "    hist.index = hist.index.date\n",
    "\n",
    "    # Create a dictionary to hold the data\n",
    "    data_dict = {date: None for date in (start_date + timedelta(n) for n in range((end_date - start_date).days + 1))}\n",
    "\n",
    "    # Update the dictionary with actual data\n",
    "    for date, row in hist.iterrows():\n",
    "        data_dict[date] = row\n",
    "\n",
    "    # Insert data into the table\n",
    "    for date in data_dict:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        if data_dict[date] is not None:\n",
    "            row = data_dict[date]\n",
    "            cursor.execute(f'''\n",
    "            INSERT OR REPLACE INTO \"{ticker}\" (Date, Open, High, Low, Close, Volume, Dividends, Stock_Splits)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (date_str, row['Open'], row['High'], row['Low'], row['Close'], \n",
    "                  row['Volume'], row['Dividends'], row['Stock Splits']))\n",
    "        else:\n",
    "            cursor.execute(f'''\n",
    "            INSERT OR REPLACE INTO \"{ticker}\" (Date, Open, High, Low, Close, Volume, Dividends, Stock_Splits)\n",
    "            VALUES (?, 0, 0, 0, 0, 0, 0, 0)\n",
    "            ''', (date_str,))\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Historical data for {ticker} has been inserted into the database.\")\n",
    "\n",
    "    # Verify the data insertion\n",
    "    cursor.execute(f'SELECT COUNT(*) FROM \"{ticker}\"')\n",
    "    row_count = cursor.fetchone()[0]\n",
    "    print(f\"Number of rows in {ticker} table: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each ticker to create tables and insert data\n",
    "for ticker in tickers:\n",
    "    specific_ticker = ticker[0]  # Extract the ticker from the tuple\n",
    "    try:\n",
    "        create_and_populate_table(specific_ticker)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {specific_ticker}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "SELECT Date, Open, High, Low, Close, Volume\n",
    "FROM \"SQQQ\"\n",
    "ORDER BY Date DESC\n",
    "LIMIT 10\n",
    "''')\n",
    "\n",
    "# Fetch the results\n",
    "results = cursor.fetchall()\n",
    "\n",
    "# Print column names\n",
    "print(\"Date\\t\\tOpen\\tHigh\\tLow\\tClose\\tVolume\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Print the first 10 rows\n",
    "for row in results:\n",
    "    print(f\"{row[0]}\\t{row[1]:.2f}\\t{row[2]:.2f}\\t{row[3]:.2f}\\t{row[4]:.2f}\\t{row[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2) Fama and French 5-factor data\n",
    "1-year time window (starting from 2023-10-28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file, skipping the first two rows of metadata\n",
    "ff5 = pd.read_csv(\"F-F_Research_Data_5_Factors_2x3_daily.CSV\", skiprows=2, index_col=0)\n",
    "\n",
    "# Convert the index to datetime \n",
    "ff5.index = pd.to_datetime(ff5.index, format='%Y%m%d')\n",
    "\n",
    "# Remove any potential whitespace in column names\n",
    "ff5.columns = ff5.columns.str.strip()\n",
    "\n",
    "# Convert data to numeric, replacing any non-numeric values with NaN\n",
    "for col in ff5.columns:\n",
    "    ff5[col] = pd.to_numeric(ff5[col], errors='coerce')\n",
    "\n",
    "# Drop observations older than 2023-10-28\n",
    "ff5 = ff5[ff5.index >= '2023-10-28']\n",
    "\n",
    "# Create a new \"Date\" column from the index\n",
    "ff5[\"Date\"] = ff5.index\n",
    "\n",
    "# Sort columns\n",
    "ff5 = ff5[['Date', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'RF']]     \n",
    "\n",
    "# Drop index\n",
    "ff5.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the first few rows of the resulting dataframe\n",
    "print(ff5.head())\n",
    "\n",
    "# Save the filtered data to a new CSV file (uncomment if needed)\n",
    "# ff5.to_csv('filtered_fama_french_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Data Analysis\n",
    "(Limited on the first 5 ETFs that are available in the Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1) Visualize ETF data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get db path\n",
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get the first 5 tickers\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' LIMIT 5\")\n",
    "tickers = [ticker[0] for ticker in cursor.fetchall()]\n",
    "\n",
    "# Function to get data for a ticker\n",
    "def get_ticker_data(ticker):\n",
    "    # First, let's check the structure of the table\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"PRAGMA table_info({ticker})\")\n",
    "    columns = [col[1] for col in cursor.fetchall()]\n",
    "    \n",
    "    # Construct the SQL query based on available columns\n",
    "    date_col = 'Date' if 'Date' in columns else 'date'\n",
    "    price_cols = [col for col in ['Open', 'High', 'Low', 'Close'] if col in columns]\n",
    "    volume_col = 'Volume' if 'Volume' in columns else None\n",
    "    \n",
    "    select_cols = [date_col] + price_cols + ([volume_col] if volume_col else [])\n",
    "    select_cols_str = ', '.join(select_cols)\n",
    "    \n",
    "    sql_query = f'''\n",
    "    SELECT {select_cols_str}\n",
    "    FROM {ticker}\n",
    "    ORDER BY {date_col} DESC\n",
    "    LIMIT 100\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_sql_query(sql_query, conn)\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df.set_index(date_col, inplace=True)\n",
    "    \n",
    "    # Rename columns if necessary\n",
    "    column_map = {'opening_price_cer': 'Open', 'daily_high_cer': 'High', \n",
    "                  'daily_low_cer': 'Low', 'closing_price_cer': 'Close'}\n",
    "    df.rename(columns=column_map, inplace=True)\n",
    "    \n",
    "    return df.sort_index()\n",
    "\n",
    "# Create plots for each ticker\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        data = get_ticker_data(ticker)\n",
    "        \n",
    "        if len(data) == 0:\n",
    "            print(f\"No data available for {ticker}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        if not all(col in data.columns for col in ['Open', 'High', 'Low', 'Close']):\n",
    "            print(f\"Missing required price columns for {ticker}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Define figure size and scale\n",
    "        figsize = (20, 10) \n",
    "        figscale = 1.5  \n",
    "\n",
    "        # Plot candlestick chart\n",
    "        mpf.plot(data, type='candle', style='charles', \n",
    "                 title=f'Price Movement of {ticker}', \n",
    "                 ylabel='Price', \n",
    "                 volume='Volume' in data.columns, \n",
    "                 figsize=figsize, \n",
    "                 figscale=figscale)\n",
    "        \n",
    "        # Save the plot as a PNG file\n",
    "        plt.savefig(f'{ticker}_candlestick.png')\n",
    "        plt.close()  # Close the plot to free up memory\n",
    "        \n",
    "        print(f\"Chart created for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {str(e)}\")\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Candlestick charts creation process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2) Computation of daily returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get db path\n",
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get all tickers (tables) except 'etf_categories'\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name != 'etf_categories' LIMIT 5\")\n",
    "# cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name != 'etf_categories'\")\n",
    "tickers = [ticker[0] for ticker in cursor.fetchall()]\n",
    "\n",
    "def calculate_and_store_daily_returns(ticker):\n",
    "    try:\n",
    "        # Check if the table has the necessary columns\n",
    "        cursor.execute(f\"PRAGMA table_info({ticker})\")\n",
    "        columns = [col[1] for col in cursor.fetchall()]\n",
    "        if 'Date' not in columns or 'Close' not in columns:\n",
    "            print(f\"Skipping {ticker}: missing required columns\")\n",
    "            return\n",
    "\n",
    "        # Fetch data\n",
    "        query = f\"SELECT Date, Close FROM {ticker} ORDER BY Date\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "        \n",
    "        # Calculate daily returns\n",
    "        df['Daily_Return'] = df['Close'].pct_change()\n",
    "        \n",
    "        # Replace infinity and NaN with NULL for SQL\n",
    "        df['Daily_Return'] = df['Daily_Return'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Check if 'Daily_Return' column exists, if not, add it\n",
    "        if 'Daily_Return' not in columns:\n",
    "            cursor.execute(f\"ALTER TABLE {ticker} ADD COLUMN Daily_Return REAL\")\n",
    "        \n",
    "        # Update the table with daily returns\n",
    "        for date, daily_return in df['Daily_Return'].items():\n",
    "            if pd.notna(daily_return):\n",
    "                cursor.execute(f\"\"\"\n",
    "                    UPDATE {ticker}\n",
    "                    SET Daily_Return = ?\n",
    "                    WHERE Date = ?\n",
    "                \"\"\", (float(daily_return), date.strftime('%Y-%m-%d')))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(f\"Daily returns calculated and stored for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {str(e)}\")\n",
    "\n",
    "# Process each ticker\n",
    "for ticker in tickers:\n",
    "    calculate_and_store_daily_returns(ticker)\n",
    "\n",
    "print(\"Daily returns have been calculated and stored for all applicable tickers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get db path\n",
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Function to check columns in the dataframe for the given ticker\n",
    "def check_columns(ticker):\n",
    "    try:\n",
    "        # Check if the table has the necessary columns\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"PRAGMA table_info({ticker})\")\n",
    "        columns = [col[1] for col in cursor.fetchall()]\n",
    "        \n",
    "        print(f\"Columns in {ticker}: {columns}\")\n",
    "        \n",
    "        # Check if DataFrame has the required columns\n",
    "        query = f\"SELECT * FROM {ticker} LIMIT 1\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        df_columns = df.columns.tolist()\n",
    "        print(f\"DataFrame columns for {ticker}: {df_columns}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking columns for {ticker}: {str(e)}\")\n",
    "\n",
    "# Specify the ticker to check\n",
    "ticker = 'SQQQ'\n",
    "check_columns(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3) Merge data of Fama and French with ETF datatables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ff5 to the database\n",
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Load ff5 into the SQLite database as a new table\n",
    "ff5.to_sql('ff5', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get db path\n",
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get all tickers (tables) except 'etf_categories'\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name != 'etf_categories'\")\n",
    "tickers = [ticker[0] for ticker in cursor.fetchall()]\n",
    "\n",
    "# Load ff5 data from the database (assuming it has been previously loaded)\n",
    "ff5 = pd.read_sql_query(\"SELECT * FROM ff5\", conn)\n",
    "ff5['Date'] = pd.to_datetime(ff5['Date'])  # Ensure Date is in datetime format\n",
    "ff5.set_index('Date', inplace=True)  # Set Date as index for merging\n",
    "\n",
    "def merge_data_with_ff5(ticker):\n",
    "    try:\n",
    "        # Fetch ETF data\n",
    "        query = f\"SELECT * FROM {ticker}\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])  # Ensure Date is in datetime format\n",
    "        df.set_index('Date', inplace=True)  # Set Date as index for merging\n",
    "\n",
    "        # Merge with ff5 data using inner join on Date\n",
    "        merged_df = pd.merge(df, ff5, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "        # Update the table with merged data\n",
    "        merged_df.reset_index(inplace=True)  # Reset index to include Date as a column\n",
    "        merged_df.to_sql(ticker, conn, if_exists='replace', index=False)\n",
    "\n",
    "        print(f\"Successfully updated {ticker} with FF5 data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {str(e)}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        print(f\"Error details: {e.args}\")\n",
    "\n",
    "# Process each ticker\n",
    "for ticker in tickers:\n",
    "    merge_data_with_ff5(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get db path\n",
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Function to get column names from a specified table\n",
    "def get_column_names(table_name):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    columns = [column[1] for column in cursor.fetchall()]\n",
    "    return columns\n",
    "\n",
    "# Check columns in the SQQQ table\n",
    "sqqq_columns = get_column_names('SQQQ')\n",
    "print(\"Columns in SQQQ table:\", sqqq_columns)\n",
    "\n",
    "# Fetch first 5 observations from the SQQQ table\n",
    "query = \"SELECT * FROM SQQQ LIMIT 5\"\n",
    "first_five_rows = pd.read_sql_query(query, conn)\n",
    "print(\"\\nFirst 5 observations in SQQQ table:\")\n",
    "print(first_five_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Set up and run Fama and French 5-Factor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get db path\n",
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get all tickers (tables) except 'etf_categories'\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name != 'etf_categories'\")\n",
    "tickers = [ticker[0] for ticker in cursor.fetchall()][:5]  # Get first 5 tickers\n",
    "\n",
    "def compute_excess_return_and_run_regression(ticker):\n",
    "    try:\n",
    "        # Fetch ETF data\n",
    "        query = f\"SELECT * FROM {ticker}\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])  # Ensure Date is in datetime format\n",
    "        df.set_index('Date', inplace=True)  # Set Date as index\n",
    "\n",
    "        # Replace NaN values in Daily_Return with 0\n",
    "        df['Daily_Return'].fillna(0, inplace=True)\n",
    "\n",
    "        # Calculate excess returns: ETF returns - Risk-free rate\n",
    "        df['Excess_Return'] = df['Daily_Return'] - df['RF_x']\n",
    "\n",
    "        # Check for NaN values in relevant columns after calculation\n",
    "        if df[['Excess_Return', 'Mkt-RF_x', 'SMB_x', 'HML_x', 'RMW_x', 'CMA_x']].isnull().any().any():\n",
    "            print(f\"NaN values found in {ticker} after calculation. Dropping rows with NaN values.\")\n",
    "            df.dropna(subset=['Excess_Return', 'Mkt-RF_x', 'SMB_x', 'HML_x', 'RMW_x', 'CMA_x'], inplace=True)\n",
    "\n",
    "        # Prepare X and y for regression\n",
    "        X = df[['Mkt-RF_x', 'SMB_x', 'HML_x', 'RMW_x', 'CMA_x']]\n",
    "        y = df['Excess_Return']\n",
    "        \n",
    "        X = sm.add_constant(X)  # Add constant term\n",
    "\n",
    "        # Run OLS regression\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        print(f\"Regression results for {ticker}:\")\n",
    "        print(model.summary())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {str(e)}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        print(f\"Error details: {e.args}\")\n",
    "\n",
    "# Run computation and regression for each ticker\n",
    "for ticker in tickers:\n",
    "    compute_excess_return_and_run_regression(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Robustness Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1) HML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get db path\n",
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get all tickers (tables) except 'etf_categories'\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name != 'etf_categories'\")\n",
    "tickers = [ticker[0] for ticker in cursor.fetchall()][:5]  # Get first 5 tickers\n",
    "\n",
    "def plot_regression_results_HML(ticker):\n",
    "    \"\"\"Plot regression diagnostics for the specified ticker.\"\"\"\n",
    "    try:\n",
    "        # Fetch data for the ticker\n",
    "        query = f\"SELECT Date, Daily_Return, HML_x FROM {ticker}\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        # Ensure Date is datetime and set it as index\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        # Drop rows with NaN values in Daily_Return or HML\n",
    "        df.dropna(subset=['Daily_Return', 'HML_x'], inplace=True)\n",
    "\n",
    "        # Define dependent and independent variables\n",
    "        X = sm.add_constant(df['HML_x'])  # Add a constant term for the intercept\n",
    "        y = df['Daily_Return']\n",
    "\n",
    "        # Fit the regression model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "\n",
    "        # Plotting residuals vs HML\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        sm.graphics.plot_regress_exog(model, 'HML_x', fig=fig)\n",
    "        plt.suptitle(f'Regression Diagnostics of HML for {ticker}', fontsize=16)\n",
    "        plt.title(f'Factor: HML', fontsize=14)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {str(e)}\")\n",
    "\n",
    "# Process each ticker and plot regression results\n",
    "for ticker in tickers:\n",
    "    plot_regression_results_HML(ticker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2) SMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get db path\n",
    "db_path = os.path.join(os.getcwd(), 'etf_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get all tickers (tables) except 'etf_categories'\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name != 'etf_categories'\")\n",
    "tickers = [ticker[0] for ticker in cursor.fetchall()][:5]  # Get first 5 tickers\n",
    "\n",
    "def plot_regression_results_HML(ticker):\n",
    "    \"\"\"Plot regression diagnostics for the specified ticker.\"\"\"\n",
    "    try:\n",
    "        # Fetch data for the ticker\n",
    "        query = f\"SELECT Date, Daily_Return, SMB_x FROM {ticker}\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        # Ensure Date is datetime and set it as index\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        # Drop rows with NaN values in Daily_Return or HML\n",
    "        df.dropna(subset=['Daily_Return', 'SMB_x'], inplace=True)\n",
    "\n",
    "        # Define dependent and independent variables\n",
    "        X = sm.add_constant(df['SMB_x'])  # Add a constant term for the intercept\n",
    "        y = df['Daily_Return']\n",
    "\n",
    "        # Fit the regression model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "\n",
    "        # Plotting residuals vs HML\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        sm.graphics.plot_regress_exog(model, 'SMB_x', fig=fig)\n",
    "        plt.suptitle(f'Regression Diagnostics of SMB for {ticker}', fontsize=16)\n",
    "        plt.title(f'Factor: SMB', fontsize=14)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {str(e)}\")\n",
    "\n",
    "# Process each ticker and plot regression results\n",
    "for ticker in tickers:\n",
    "    plot_regression_results_HML(ticker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Same plots can be generated also for Mkt-RF, 'RMW' and 'CMA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def process_data(file_name, date_format='%d.%m.%y', delimiter=';'):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_name, delimiter=delimiter, na_values=[\"\", \"NA\", \"N/A\", \"tbd\"])\n",
    "    \n",
    "    # Rename columns\n",
    "    column_mapping = {\n",
    "        'Date': 'Date', 'Datum': 'Date',\n",
    "        'Close': 'Close', 'Schluss': 'Close',\n",
    "        'Open': 'Open', 'Er√∂ffnung': 'Open',\n",
    "        'Daily_high': 'High', 'Tageshoch': 'High',\n",
    "        'Daily_low': 'Low', 'Tagestief': 'Low'\n",
    "    }\n",
    "    df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    # Process dates and set index\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format=date_format)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df = df.iloc[::-1]\n",
    "    \n",
    "    # Convert to numeric and handle missing data\n",
    "    numeric_columns = ['Close', 'Open', 'High', 'Low']\n",
    "    for col in numeric_columns:\n",
    "        df[col] = pd.to_numeric(df[col].astype(str).str.replace('.', '').str.replace(',', '.'), errors='coerce')\n",
    "    \n",
    "    df = df.dropna(subset=numeric_columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_last_100(df, title):\n",
    "    df100 = df.tail(100)\n",
    "    mpf.plot(df100[['Open', 'High', 'Low', 'Close']], type='candle', style='charles', \n",
    "             title=f'Price Movement of {title} (last 100 days)', \n",
    "             ylabel='Price (EUR)', volume=False, figsize=(20, 10), figscale=1.5)\n",
    "    plt.show()\n",
    "\n",
    "# Process each dataset\n",
    "eustx = process_data(\"__data/EUROSTOXX50.csv\")\n",
    "sp500 = process_data(\"__data/S&P500.csv\")\n",
    "msciWorld = process_data(\"__data/MSCIWorld.csv\")\n",
    "\n",
    "# Plot last 100 observations for each dataset\n",
    "plot_last_100(eustx, \"EUROSTOXX50\")\n",
    "plot_last_100(sp500, \"S&P 500\")\n",
    "plot_last_100(msciWorld, \"MSCI World Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge data\n",
    "def process_and_rename(df, prefix):\n",
    "    # Ensure 'Date' is a column, not an index\n",
    "    if 'Date' not in df.columns:\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    rename_dict = {\n",
    "        'Open': f'open_{prefix}',\n",
    "        'High': f'high_{prefix}',\n",
    "        'Low': f'low_{prefix}',\n",
    "        'Close': f'close_{prefix}'\n",
    "    }\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    \n",
    "    # Ensure Date is datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    return df[['Date'] + list(rename_dict.values())]\n",
    "\n",
    "# Process and rename other datasets\n",
    "eustx = process_and_rename(eustx, 'eustx')\n",
    "sp500 = process_and_rename(sp500, 'sp500')\n",
    "msciWorld = process_and_rename(msciWorld, 'msci')\n",
    "\n",
    "# Merge all datasets\n",
    "data = pd.merge(eustx, ff5, on='Date', how='inner')\n",
    "data = pd.merge(data, sp500, on='Date', how='inner')\n",
    "data = pd.merge(data, msciWorld, on='Date', how='inner')\n",
    "\n",
    "# Display the first few rows and column names of the resulting dataframe\n",
    "print(data.head())\n",
    "print(\"\\nColumns in the merged dataset:\")\n",
    "print(data.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
